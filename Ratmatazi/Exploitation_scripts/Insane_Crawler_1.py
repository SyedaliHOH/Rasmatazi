import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

class Crawler:
    def __init__(self, url, depth=3):
        self.url = url
        self.depth = depth
        self.internal_links = set()
        self.external_links = set()

    def crawl(self):
        self._crawl(self.url, self.depth)

    def _crawl(self, url, depth):
        if depth == 0:
            return
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        for tag in soup.find_all(['a', 'link', 'script']):
            if tag.has_attr('href'):
                href = tag['href']
            elif tag.has_attr('src'):
                href = tag['src']
            else:
                continue
            if href.startswith('/'):
                href = urljoin(url, href)
            parsed_href = urlparse(href)
            if parsed_href.netloc == urlparse(url).netloc:
                self.internal_links.add(href)
            else:
                self.external_links.add(href)
        for link in self.internal_links.copy():
            self._crawl(link, depth - 1)

    def print_links(self, output_file=None, separate_files=False):
        if output_file:
            if separate_files:
                with open(output_file + "_internal.txt", 'w') as f:
                    for link in self.internal_links:
                        f.write(link + "\n")
                with open(output_file + "_external.txt", 'w') as f:
                    for link in self.external_links:
                        f.write(link + "\n")
            else:
                with open(output_file, 'w') as f:
                    f.write("Internal links:\n")
                    for link in self.internal_links:
                        f.write(link + "\n")
                    f.write("\nExternal links:\n")
                    for link in self.external_links:
                        f.write(link + "\n")
        else:
            print("Internal links:")
            for link in self.internal_links:
                print(link)
            print("\nExternal links:")
            for link in self.external_links:
                print(link)

    def check_internal_links(self, output_file):
        with open('test_internal.txt', 'r') as f:
            internal_links = [line.strip() for line in f.readlines()]

        alive_links = []
        for link in internal_links:
            try:
                response = requests.get(link)
                if response.status_code != 404:
                    alive_links.append((link, response.status_code, len(response.content)))
            except requests.exceptions.RequestException as e:
                print(f"Error requesting {link}: {e}")

        with open(output_file, 'w') as f:
            for link, status_code, length in alive_links:
                f.write(f"{link} {status_code} {length}\n")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("url", help="URL to crawl")
    parser.add_argument("-d", "--depth", type=int, default=3, help="Crawl depth")
    parser.add_argument("-o", "--output", help="Output file")
    parser.add_argument("-os", "--output_separate", action="store_true", help="Output separate files for internal and external links")
    parser.add_argument("-c", "--check_internal", action="store_true", help="Check internal links and output to alive.txt")
    args = parser.parse_args()
    crawler = Crawler(args.url, args.depth)
    crawler.crawl()
    if args.output:
        crawler.print_links(args.output, separate_files=args.output_separate)
    if args.check_internal:
        crawler.check_internal_links('alive.txt')