#!/usr/bin/env python3

import argparse
import os
import subprocess
import json

# Default values for optional parameters
DEFAULT_OUTPUT_DIR = "."
DEFAULT_THREADS = 8
DEFAULT_DEPTH = 5
EXCLUDE_DOMAINS = [
    "code.jquery.com", "cdn.jsdelivr.net", "stackpath.bootstrapcdn.com",
    "cdnjs.cloudflare.com", "maxcdn.bootstrapcdn.com", "unpkg.com"
]

def should_exclude(link):
    """Check if a domain should be excluded from SecretFinder scanning."""
    return any(domain in link for domain in EXCLUDE_DOMAINS)

def run_hakrawler(target_url, threads, depth, proxy, disable_redirects, insecure, custom_headers):
    """Run hakrawler and return crawled links."""
    cmd = [
        "hakrawler",
        "-t", str(threads),
        "-d", str(depth)
    ]
    if disable_redirects:
        cmd.append("-dr")
    if insecure:
        cmd.append("-insecure")
    if proxy:
        print("[W] Selecting proxy may not return results.")  # Warning added here
        cmd.extend(["-proxy", proxy])
    if custom_headers:
        cmd.extend(["-h", custom_headers])

    print(f"Running hakrawler with: {cmd}")
    try:
        result = subprocess.run(cmd, input=target_url, text=True, capture_output=True)
        if result.returncode != 0:
            print(f"Error running hakrawler: {result.stderr}")
            exit(1)
        return result.stdout.strip().split("\n")
    except FileNotFoundError:
        print("Error: hakrawler not found. Please ensure it is installed and in your PATH.")
        exit(1)

def run_secretfinder(js_links, output_file, proxy, custom_headers):
    """Run SecretFinder on the given JavaScript links."""
    with open(output_file, "w") as f:
        for link in js_links:
            print(f"[+] Scanning remote JS URL: {link}")
            cmd = [
                "python3", "../SecretFinder/SecretFinder.py",
                "-i", link,
                "-o", "cli",
            ]
            if proxy:
                cmd.extend(["-p", proxy])  # Add proxy only if provided
            if custom_headers:
                formatted_headers = custom_headers.replace("; ", "\n")
                cmd.extend(["-H", formatted_headers])  # Pass custom headers
            # print(cmd)
            # Set the environment variables explicitly for proxy usage
            # Explicitly set environment variables for proxy
            env = os.environ.copy()
            if proxy:
                env["HTTP_PROXY"] = proxy
                env["HTTPS_PROXY"] = proxy
                env["http_proxy"] = proxy
                env["https_proxy"] = proxy  # For case sensitivity in some systems

            try:
                result = subprocess.run(
                    cmd, 
                    capture_output=True, 
                    text=True, 
                    check=True, 
                    timeout=30, 
                    env=env
                )
                # print(f"Command Output: {result.stdout}")
                # print(f"Command Error (if any): {result.stderr}")
                if result.returncode == 0:
                    f.write(result.stdout)
                else:
                    print(f"Error scanning {link}: {result.stderr}")
            except subprocess.TimeoutExpired:
                print(f"Timeout expired while scanning {link}")
            except subprocess.CalledProcessError as e:
                print(f"Error scanning {link}: {e.stderr}")

def save_as_json(output_dir, internal_links, external_links, filtered_links):
    """Save categorized links into a JSON file."""
    json_file = os.path.join(output_dir, "crawled_links.json")
    data = {
        "internal_links": internal_links,
        "external_links": external_links,
        "filtered_links": filtered_links,
    }
    with open(json_file, "w") as f:
        json.dump(data, f, indent=4)
    print(f"Crawled links JSON saved in: {json_file}")

def main():
    parser = argparse.ArgumentParser(description="Crawl a target URL and run SecretFinder.")
    parser.add_argument("-u", "--url", required=True, help="Target URL to crawl")
    parser.add_argument("-o", "--output", default=DEFAULT_OUTPUT_DIR, help="Output directory for files")
    parser.add_argument("-t", "--threads", type=int, default=DEFAULT_THREADS, help="Threads for hakrawler")
    parser.add_argument("-d", "--depth", type=int, default=DEFAULT_DEPTH, help="Crawl depth for hakrawler")
    parser.add_argument("-p", "--proxy", help="Proxy server (e.g., http://127.0.0.1:8080)")
    parser.add_argument("-dr", "--disable-redirects", action="store_true", help="Disable redirects in hakrawler")
    parser.add_argument("-insecure", action="store_true", help="Disable TLS verification")
    parser.add_argument("-H", "--headers", help="Custom headers (e.g., 'Header1: Value1; Header2: Value2')")
    parser.add_argument("-scan-all", action="store_true", help="Scan all JS files, including external ones")
    
    args = parser.parse_args()

    # Prepare output directory
    os.makedirs(args.output, exist_ok=True)

    # Run hakrawler with -sort -u for unique results
    crawled_links = run_hakrawler(
        target_url=args.url,
        threads=args.threads,
        depth=args.depth,
        proxy=args.proxy,
        disable_redirects=args.disable_redirects,
        insecure=args.insecure,
        custom_headers=args.headers
    )

    # Filter unique links using the -sort -u option
    crawled_links = list(set(crawled_links))  # Remove duplicates
    print(f"[+] Unique crawled links: {len(crawled_links)}")

    # Save crawled links
    fullcrawl_file = os.path.join(args.output, "fullcrawl.txt")
    with open(fullcrawl_file, "w") as f:
        f.write("\n".join(crawled_links))
    print(f"[S] All crawled links saved in: {fullcrawl_file}")

    # Categorize links
    internal_links = [link for link in crawled_links if link.startswith(args.url)]
    external_links = [link for link in crawled_links if link.startswith("http") and not link.startswith(args.url)]
    filtered_links = [link for link in internal_links if link.endswith(".js") and not should_exclude(link)]

    # If -scan-all is set, include external JS links in the scanning
    if args.scan_all:
        filtered_links += [link for link in external_links if link.endswith(".js")]

    # Save filtered links
    filtered_links_file = os.path.join(args.output, "filtered_js_links.txt")
    with open(filtered_links_file, "w") as f:
        f.write("\n".join(filtered_links))
    print(f"[S] Filtered JS links saved in: {filtered_links_file}")

    # Save links to JSON
    save_as_json(args.output, internal_links, external_links, filtered_links)

    # Run SecretFinder on filtered links
    secretfinder_output = os.path.join(args.output, "secretfinder_results.txt")
    run_secretfinder(
        js_links=filtered_links,
        output_file=secretfinder_output,
        proxy=args.proxy,
        custom_headers=args.headers
    )
    print(f"[S] SecretFinder results saved in: {secretfinder_output}")

if __name__ == "__main__":
    main()
